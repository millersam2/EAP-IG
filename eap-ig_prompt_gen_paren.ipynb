{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA A100-SXM4-80GB MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# for writing dataset out to data.json\n",
    "import pandas as pd\n",
    "import json\n",
    "# for debugging purposes\n",
    "import sys\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "# check for GPU\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to Modify Prompts Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of desired model\n",
    "# index 0 - codellama\n",
    "# index 1 - Phi3\n",
    "# index 2 - StarCoder2\n",
    "# index 3 - DeepSeek Coder\n",
    "index = 0\n",
    "\n",
    "# the integer values inside the parens\n",
    "nums = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    12,\n",
    "    23,\n",
    "    123,\n",
    "    123456789\n",
    "]\n",
    "\n",
    "# NOTE: clean, corrupted prompts pair generation currently only supported for str\n",
    "# constructor calls to be used for the prompts\n",
    "constructor_calls = [\n",
    "    \"str\",\n",
    "    # \"list\",\n",
    "    # \"set\",\n",
    "    # \"float\",\n",
    "    # \"chr\",\n",
    "    # \"bin\"\n",
    "]\n",
    "\n",
    "# number of constructor calls to be included in the prompts\n",
    "# for sequence constructor calls (list, set): \n",
    "# the number of constructor calls + 2 = number of parentheses\n",
    "# for basic type constructor calls (str, float, chr, bin):\n",
    "# the number of constructor calls + 1 = number of parentheses\n",
    "num_constructors = [\n",
    "    # 1,\n",
    "    # 2,\n",
    "    # 3,\n",
    "    # 4,\n",
    "    5,\n",
    "    # 6,\n",
    "    # 7,\n",
    "    # 10,\n",
    "]\n",
    "\n",
    "# number of right parentheses to remove from the corrupted prompt\n",
    "# its correct token is simply the token the prompt generates\n",
    "corrupted_remove_parens = 2\n",
    "corrupted_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(i):\n",
    "    projects_dir = \"/projects/ziyuyao\"\n",
    "    cache_loc1 = projects_dir + \"/codellama/codellama-7b\"\n",
    "    cache_loc2 = projects_dir + \"/phi3\"\n",
    "    cache_loc3 = projects_dir + \"/StarCoder2\"\n",
    "    cache_loc4 = projects_dir + \"/DeepSeekCoder\"\n",
    "\n",
    "    model_args = [\n",
    "        (\"codellama/CodeLlama-7b-hf\", cache_loc1, False, torch.float16),\n",
    "        (\"microsoft/Phi-3-mini-4k-instruct\", cache_loc2, True, \"auto\"),\n",
    "        (\"bigcode/starcoder2-7b\", cache_loc3, False, torch.bfloat16),\n",
    "        (\"deepseek-ai/deepseek-coder-6.7b-base\", cache_loc4, True, torch.bfloat16),\n",
    "    ]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args[i][0], cache_dir=model_args[i][1], trust_remote_code=model_args[i][2])\n",
    "\n",
    "    # torch.float16 or torch.bfloat16 used to fit onto 20gb GPU\n",
    "    if i == 1:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_args[i][0], cache_dir=model_args[i][1], trust_remote_code=model_args[i][2], torch_dtype=model_args[i][3], attn_implementation='eager')\n",
    "    else: \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_args[i][0], cache_dir=model_args[i][1], trust_remote_code=model_args[i][2], torch_dtype=model_args[i][3])\n",
    "\n",
    "    return (model, tokenizer)\n",
    "\n",
    "\n",
    "def create_prompts(corrupted_flag=False, corrupted_num_parens=0):\n",
    "    prompts_dict = dict()\n",
    "\n",
    "    for cc in constructor_calls:\n",
    "        if cc == 'str':\n",
    "            prompts_dict[cc] = [\n",
    "                f'#print a string {num}\\n' + 'print(' + (cc + '(') * x + str(num) + ')' * (x + 1) \n",
    "                for x in num_constructors \n",
    "                for num in nums\n",
    "            ]\n",
    "        else: # cc is 'list' or 'set'\n",
    "            prompts_dict[cc] = [\n",
    "                f'#print a {cc} containing {num}\\n'+ 'print(' + (cc + '(') * x + 'tuple([' + str(num) + '])' + ')' * (x + 1) \n",
    "                for x in num_constructors \n",
    "                for num in nums\n",
    "            ]\n",
    "\n",
    "    count = 0\n",
    "    # list of tuples (prompt, correct next token)\n",
    "    clean_prompts = []\n",
    "    corrupted_prompts = []\n",
    "    for cc in constructor_calls:\n",
    "        for j in range(0, len(prompts_dict[cc])):\n",
    "            tokens = tokenizer.tokenize(prompts_dict[cc][j])\n",
    "            prompt = prompts_dict[cc][j]\n",
    "            correct_nt = tokens[-1]\n",
    "            prompt = prompt[:len(prompt) - len(correct_nt)]\n",
    "\n",
    "            if corrupted_flag:\n",
    "                corrupted_prompt = prompt[:len(prompt) - corrupted_num_parens]\n",
    "                corrupted_tokens = tokenizer.tokenize(corrupted_prompt)\n",
    "                # check that the clean and corrupted prompts have the same number of tokens\n",
    "                if len(tokens) - 1 == len(corrupted_tokens):\n",
    "                    inputs = tokenizer(corrupted_prompt, return_tensors='pt').to(\"cuda\")\n",
    "                    nt_logits = model(**inputs)['logits'][:, -1, :]\n",
    "                    corrupted_correct_nt = tokenizer.decode(torch.argmax(nt_logits).item())\n",
    "                    print(f'Prompt: \"{corrupted_prompt}\"')\n",
    "                    print(f'Counterfactual Next Token: \"{corrupted_correct_nt}\"')\n",
    "                    clean_prompts.append((prompt, correct_nt))\n",
    "                    corrupted_prompts.append((corrupted_prompt, corrupted_correct_nt, torch.argmax(nt_logits).item()))\n",
    "                    count+=1\n",
    "            else:\n",
    "                clean_prompts.append((prompt, correct_nt))\n",
    "                count+=1        \n",
    "\n",
    "    print(f'Total {\"Clean/Corrupted Prompt Pairs\" if corrupted_flag else \"Clean Prompts\"}: {count}')\n",
    "    prompts = [\n",
    "        {'clean_prompt_tuple':clean_prompts[i], 'corrupted_prompt_tuple':corrupted_prompts[i] if corrupted_flag else None,} \n",
    "        for i in range(0, len(clean_prompts))\n",
    "    ]\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def populate_validate_dict_entry(d, idx, prompt_pair, current_output, clean_and_corrupted_flag=False):\n",
    "    d['index'].append(idx)\n",
    "    d['clean_prompts'].append(prompt_pair['clean_prompt_tuple'][0])\n",
    "    d['clean_correct_outputs'].append(prompt_pair['clean_prompt_tuple'][1])\n",
    "    d['clean_current_outputs'].append(current_output)\n",
    "\n",
    "    if clean_and_corrupted_flag:\n",
    "        d['corrupted_prompts'].append(prompt_pair['corrupted_prompt_tuple'][0])\n",
    "        d['corrupted_correct_outputs'].append(prompt_pair['corrupted_prompt_tuple'][1])\n",
    "        d['corrupted_correct_token_ids'].append(prompt_pair['corrupted_prompt_tuple'][2])\n",
    "    # return d\n",
    "\n",
    "def validate_prompts(model, tokenizer, prompts, clean_and_corrupted_flag=False):\n",
    "    dict_keys = ['index', 'clean_prompts', 'clean_correct_outputs', 'clean_current_outputs', 'corrupted_prompts', 'corrupted_correct_outputs', 'corrupted_correct_token_ids']\n",
    "    \n",
    "    d = {key: [] for key in dict_keys + ['clean_correct_token_ids']}\n",
    "    incorrect_d = {key: [] for key in dict_keys + ['clean_current_token_ids']}\n",
    "\n",
    "    # prompts = [('#print a string 2\\nprint(str(str(str(str(str(str(str(str(str(str(2', ')))')]\n",
    "    for idx, prompt_pair in enumerate(prompts):\n",
    "        inputs = tokenizer(prompt_pair['clean_prompt_tuple'][0], return_tensors='pt').to(\"cuda\")\n",
    "        nt_logits = model(**inputs)['logits'][:, -1, :]\n",
    "        current_token_id = torch.argmax(nt_logits).item()\n",
    "        # \n",
    "        current_output = tokenizer.decode(current_token_id)\n",
    "        correct_output = prompt_pair['clean_prompt_tuple'][1]\n",
    "        correct_bool = correct_output == current_output\n",
    "\n",
    "        populate_validate_dict_entry(d if correct_bool else incorrect_d, idx, prompt_pair, current_output, True)\n",
    "        if correct_bool:\n",
    "            d['clean_correct_token_ids'].append(current_token_id)\n",
    "        else:\n",
    "            # finds the top logit instance of the correct answer token\n",
    "            sorted_out, token_idxs = torch.sort(\n",
    "                nt_logits,\n",
    "                dim=-1,\n",
    "                descending=True\n",
    "            )\n",
    "            for j in range(0, sorted_out.shape[1]):\n",
    "                token = tokenizer.decode(token_idxs[0, j])\n",
    "                if correct_output == token:\n",
    "                    incorrect_d['clean_current_token_ids'].append(token_idxs[0, j].item())\n",
    "                    break\n",
    "\n",
    "            print('INCORRECT')\n",
    "            print(f'Clean Prompt {idx}: {prompt_tuple[0]}')\n",
    "            print(f'correct output = {correct_output}')\n",
    "            print(f'current output = {current_output}')\n",
    "            print('Full Generation: \"' + prompt_tuple[0] + filling + '\"')\n",
    "    return (d, incorrect_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18ad88855c049348045408dfa0eb0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"#print a string 1\n",
      "print(str(str(str(str(str(1))\"\n",
      "Counterfactual Next Token: \")))\"\n",
      "Prompt: \"#print a string 2\n",
      "print(str(str(str(str(str(2))\"\n",
      "Counterfactual Next Token: \")))\"\n",
      "Prompt: \"#print a string 3\n",
      "print(str(str(str(str(str(3))\"\n",
      "Counterfactual Next Token: \")))\"\n",
      "Prompt: \"#print a string 12\n",
      "print(str(str(str(str(str(12))\"\n",
      "Counterfactual Next Token: \")))\"\n",
      "Prompt: \"#print a string 23\n",
      "print(str(str(str(str(str(23))\"\n",
      "Counterfactual Next Token: \")))\"\n",
      "Prompt: \"#print a string 123\n",
      "print(str(str(str(str(str(123))\"\n",
      "Counterfactual Next Token: \")))\"\n",
      "Prompt: \"#print a string 123456789\n",
      "print(str(str(str(str(str(123456789))\"\n",
      "Counterfactual Next Token: \")))\"\n",
      "Total Clean/Corrupted Prompt Pairs: 7\n",
      "{'index': [0, 1, 2, 3, 4, 5, 6], 'clean_prompts': ['#print a string 1\\nprint(str(str(str(str(str(1))))', '#print a string 2\\nprint(str(str(str(str(str(2))))', '#print a string 3\\nprint(str(str(str(str(str(3))))', '#print a string 12\\nprint(str(str(str(str(str(12))))', '#print a string 23\\nprint(str(str(str(str(str(23))))', '#print a string 123\\nprint(str(str(str(str(str(123))))', '#print a string 123456789\\nprint(str(str(str(str(str(123456789))))'], 'clean_correct_outputs': ['))', '))', '))', '))', '))', '))', '))'], 'clean_current_outputs': ['))', '))', '))', '))', '))', '))', '))'], 'corrupted_prompts': ['#print a string 1\\nprint(str(str(str(str(str(1))', '#print a string 2\\nprint(str(str(str(str(str(2))', '#print a string 3\\nprint(str(str(str(str(str(3))', '#print a string 12\\nprint(str(str(str(str(str(12))', '#print a string 23\\nprint(str(str(str(str(str(23))', '#print a string 123\\nprint(str(str(str(str(str(123))', '#print a string 123456789\\nprint(str(str(str(str(str(123456789))'], 'corrupted_correct_outputs': [')))', ')))', ')))', ')))', ')))', ')))', ')))'], 'corrupted_correct_token_ids': [4961, 4961, 4961, 4961, 4961, 4961, 4961], 'clean_correct_token_ids': [876, 876, 876, 876, 876, 876, 876]}\n",
      "{'index': [], 'clean_prompts': [], 'clean_correct_outputs': [], 'clean_current_outputs': [], 'corrupted_prompts': [], 'corrupted_correct_outputs': [], 'corrupted_correct_token_ids': [], 'clean_current_token_ids': []}\n",
      "True\n",
      "clean_correct_outputs\n",
      "))    7\n",
      "Name: count, dtype: int64\n",
      "Series([], Name: count, dtype: int64)\n",
      "                                               clean  \\\n",
      "0  #print a string 1\\nprint(str(str(str(str(str(1...   \n",
      "1  #print a string 2\\nprint(str(str(str(str(str(2...   \n",
      "2  #print a string 3\\nprint(str(str(str(str(str(3...   \n",
      "3  #print a string 12\\nprint(str(str(str(str(str(...   \n",
      "4  #print a string 23\\nprint(str(str(str(str(str(...   \n",
      "\n",
      "                                           corrupted  label  \\\n",
      "0   #print a string 1\\nprint(str(str(str(str(str(1))    876   \n",
      "1   #print a string 2\\nprint(str(str(str(str(str(2))    876   \n",
      "2   #print a string 3\\nprint(str(str(str(str(str(3))    876   \n",
      "3  #print a string 12\\nprint(str(str(str(str(str(...    876   \n",
      "4  #print a string 23\\nprint(str(str(str(str(str(...    876   \n",
      "\n",
      "   counterfactual_label  \n",
      "0                  4961  \n",
      "1                  4961  \n",
      "2                  4961  \n",
      "3                  4961  \n",
      "4                  4961  \n",
      "                                               clean  \\\n",
      "0  #print a string 1\\nprint(str(str(str(str(str(1...   \n",
      "1  #print a string 2\\nprint(str(str(str(str(str(2...   \n",
      "2  #print a string 3\\nprint(str(str(str(str(str(3...   \n",
      "3  #print a string 12\\nprint(str(str(str(str(str(...   \n",
      "4  #print a string 23\\nprint(str(str(str(str(str(...   \n",
      "\n",
      "                                           corrupted  label  \\\n",
      "0   #print a string 1\\nprint(str(str(str(str(str(1))    876   \n",
      "1   #print a string 2\\nprint(str(str(str(str(str(2))    876   \n",
      "2   #print a string 3\\nprint(str(str(str(str(str(3))    876   \n",
      "3  #print a string 12\\nprint(str(str(str(str(str(...    876   \n",
      "4  #print a string 23\\nprint(str(str(str(str(str(...    876   \n",
      "\n",
      "   counterfactual_label  \n",
      "0                  4961  \n",
      "1                  4961  \n",
      "2                  4961  \n",
      "3                  4961  \n",
      "4                  4961  \n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = setup_model_and_tokenizer(index)\n",
    "model.to('cuda')\n",
    "prompts = create_prompts(corrupted_flag=True, corrupted_num_parens=corrupted_remove_parens)\n",
    "d, incorrect_d = validate_prompts(model, tokenizer, prompts, clean_and_corrupted_flag=True)\n",
    "corr_df = pd.DataFrame(d)\n",
    "print(d)\n",
    "print(incorrect_d)\n",
    "\n",
    "incorr_df = pd.DataFrame(incorrect_d)\n",
    "print(corr_df.loc[:, \"clean_correct_outputs\"].equals(corr_df.loc[:, \"clean_current_outputs\"]))\n",
    "\n",
    "corr_df.sort_values('clean_correct_token_ids', inplace=True, kind='mergesort')\n",
    "incorr_df.sort_values('clean_current_token_ids', inplace=True, kind='mergesort')\n",
    "\n",
    "print(corr_df.value_counts('clean_correct_outputs'))\n",
    "print(incorr_df.value_counts('clean_correct_outputs'))\n",
    "\n",
    "rename_dict = {\n",
    "    \"clean_prompts\": \"clean\", \n",
    "    \"corrupted_prompts\": \"corrupted\", \n",
    "    \"clean_correct_token_ids\": \"label\", \n",
    "    \"corrupted_correct_token_ids\": \"counterfactual_label\"\n",
    "    }\n",
    "corr_df.rename(columns=rename_dict, inplace=True)\n",
    "keep_columns = list(rename_dict.values())\n",
    "corr_df = corr_df[keep_columns]\n",
    "\n",
    "incorr_df.rename(columns=rename_dict, inplace=True)\n",
    "incorr_df = corr_df[keep_columns]\n",
    "\n",
    "print(corr_df.head())\n",
    "print(incorr_df.head())\n",
    "\n",
    "if len(corr_df.index):\n",
    "    corr_df.to_csv('correct_paren_data.csv', index=False)\n",
    "\n",
    "# if len(incorr_df.index):\n",
    "#     incorr_df.to_csv('incorrect_paren_data.csv', index=False)\n",
    "\n",
    "# sys.exit(1)\n",
    "\n",
    "# # convert to json and write out\n",
    "# final_corr_dataset = corr_df.to_dict('records')\n",
    "# final_incorr_dataset = incorr_df.to_dict('records')\n",
    "\n",
    "# with open(\"../../data/codellama_incorrect_paren_data_v2.json\", \"w\") as f:\n",
    "#     json.dump(final_incorr_dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# with open(\"../../data/codellama_paren_data_v2.json\", \"w\") as f:\n",
    "#     json.dump(final_corr_dataset, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
