{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA A100-SXM4-80GB MIG 2g.20gb\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# for writing dataset out to data.json\n",
    "import pandas as pd\n",
    "import json\n",
    "# for debugging purposes\n",
    "import sys\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "# check for GPU\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to Modify Prompts Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of desired model\n",
    "# index 0 - codellama\n",
    "# index 1 - Phi3\n",
    "# index 2 - StarCoder2\n",
    "# index 3 - DeepSeek Coder\n",
    "index = 0\n",
    "\n",
    "# the integer values inside the parens\n",
    "nums = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    12,\n",
    "    23,\n",
    "    123,\n",
    "    123456789\n",
    "]\n",
    "\n",
    "# NOTE: clean, corrupted prompts pair generation currently only supported for str\n",
    "# constructor calls to be used for the prompts\n",
    "constructor_calls = [\n",
    "    \"str\",\n",
    "    # \"list\",\n",
    "    # \"set\",\n",
    "    # \"float\",\n",
    "    # \"chr\",\n",
    "    # \"bin\"\n",
    "]\n",
    "\n",
    "# number of constructor calls to be included in the prompts\n",
    "# for sequence constructor calls (list, set): \n",
    "# the number of constructor calls + 2 = number of parentheses\n",
    "# for basic type constructor calls (str, float, chr, bin):\n",
    "# the number of constructor calls + 1 = number of parentheses\n",
    "num_constructors = [\n",
    "    # 1,\n",
    "    # 2,\n",
    "    # 3,\n",
    "    # 4,\n",
    "    5,\n",
    "    # 6,\n",
    "    # 7,\n",
    "    # 10,\n",
    "]\n",
    "\n",
    "# number of right parentheses to remove from the corrupted prompt\n",
    "# its correct token is simply the token the prompt generates\n",
    "corrupted_remove_parens = 2\n",
    "corrupted_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(i):\n",
    "    projects_dir = \"/projects/ziyuyao\"\n",
    "    cache_loc1 = projects_dir + \"/codellama/codellama-7b\"\n",
    "    cache_loc2 = projects_dir + \"/phi3\"\n",
    "    cache_loc3 = projects_dir + \"/StarCoder2\"\n",
    "    cache_loc4 = projects_dir + \"/DeepSeekCoder\"\n",
    "\n",
    "    model_args = [\n",
    "        (\"codellama/CodeLlama-7b-hf\", cache_loc1, False, torch.float16),\n",
    "        (\"microsoft/Phi-3-mini-4k-instruct\", cache_loc2, True, \"auto\"),\n",
    "        (\"bigcode/starcoder2-7b\", cache_loc3, False, torch.bfloat16),\n",
    "        (\"deepseek-ai/deepseek-coder-6.7b-base\", cache_loc4, True, torch.bfloat16),\n",
    "    ]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args[i][0], cache_dir=model_args[i][1], trust_remote_code=model_args[i][2])\n",
    "\n",
    "    # torch.float16 or torch.bfloat16 used to fit onto 20gb GPU\n",
    "    if i == 1:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_args[i][0], cache_dir=model_args[i][1], trust_remote_code=model_args[i][2], torch_dtype=model_args[i][3], attn_implementation='eager')\n",
    "    else: \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_args[i][0], cache_dir=model_args[i][1], trust_remote_code=model_args[i][2], torch_dtype=model_args[i][3])\n",
    "\n",
    "    return (model, tokenizer)\n",
    "\n",
    "\n",
    "def create_prompts(corrupted_flag=False, corrupted_num_parens=0):\n",
    "    prompts_dict = dict()\n",
    "\n",
    "    for cc in constructor_calls:\n",
    "        if cc == 'str':\n",
    "            prompts_dict[cc] = [f'#print a string {num}\\n' + 'print(' + (cc + '(') * x + str(num) + ')' * (x + 1) \n",
    "            for x in num_constructors \n",
    "            for num in nums]\n",
    "        else: # cc is 'list' or 'set'\n",
    "            prompts_dict[cc] = [f'#print a {cc} containing {num}\\n'+ 'print(' + (cc + '(') * x + 'tuple([' + str(num) + '])' + ')' * (x + 1) for x in num_constructors for num in nums]\n",
    "\n",
    "    count = 0\n",
    "    # list of tuples (prompt, correct next token)\n",
    "    clean_prompts = []\n",
    "    corrupted_prompts = []\n",
    "    for cc in constructor_calls:\n",
    "        for j in range(0, len(prompts_dict[cc])):\n",
    "            tokens = tokenizer.tokenize(prompts_dict[cc][j])\n",
    "            prompt = prompts_dict[cc][j]\n",
    "            correct_nt = tokens[-1]\n",
    "\n",
    "            if corrupted_flag:\n",
    "                corrupted_prompt = prompt[:len(prompt) - corrupted_num_parens]\n",
    "                corrupted_tokens = tokenizer.tokenize(corrupted_prompt)\n",
    "                # check that the clean and corrupted prompts have the same number of tokens\n",
    "                if len(tokens) == len(corrupted_tokens):\n",
    "                    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "                    nt_logits = model(**inputs)['logits'][:, -1, :]\n",
    "                    corrupted_correct_nt = tokenizer.decode(torch.argmax(nt_logits).item())\n",
    "                    print(f'Prompt: \"{corrupted_prompt}\"')\n",
    "                    print(f'Counterfactual Next Token: \"{corrupted_correct_nt}\"')\n",
    "                    clean_prompts.append((prompt[:len(prompt) - len(correct_nt)], correct_nt))\n",
    "                    corrupted_prompts.append((corrupted_prompt, corrupted_correct_nt))\n",
    "                    count+=1\n",
    "            else:\n",
    "                clean_prompts.append((prompt[:len(prompt) - len(correct_nt)], correct_nt))\n",
    "                count+=1        \n",
    "\n",
    "    print(f'Total {'Clean/Corrupted Prompt Pairs' if corrupted_flag else 'Clean Prompts'}: {count}')\n",
    "    return (clean_prompts, corrupted_prompts)\n",
    "\n",
    "\n",
    "def validate_prompts(model, tokenizer, prompts):\n",
    "    d = {'index' : [], 'prompts' : [], 'correct_outputs': [], 'current_outputs': [], 'correct_token_ids': []}\n",
    "    incorrect_d = {'index' : [], 'prompts' : [], 'correct_outputs': [], 'current_outputs': [], 'current_token_ids': [], 'correct_token_ids' : []}\n",
    "\n",
    "    # prompts = [('#print a string 2\\nprint(str(str(str(str(str(str(str(str(str(str(2', ')))')]\n",
    "    for idx, prompt_tuple in enumerate(prompts):\n",
    "        model_inputs = tokenizer(prompt_tuple[0], return_tensors=\"pt\").to(\"cuda\")\n",
    "        output_dict = model.generate(\n",
    "            **model_inputs, \n",
    "            max_new_tokens=5,\n",
    "            return_dict_in_generate=True,\n",
    "            output_logits=True\n",
    "        )\n",
    "        generated_ids = output_dict['sequences']\n",
    "        # https://huggingface.co/docs/transformers/en/model_doc/code_llama\n",
    "        filling = tokenizer.batch_decode(generated_ids[:, model_inputs['input_ids'].shape[1]:], skip_special_tokens = False)[0]\n",
    "        model_inputs = model_inputs.to(\"cpu\")\n",
    "        current_token_id = generated_ids[:, model_inputs['input_ids'].shape[1]]\n",
    "        current_output = tokenizer.decode(current_token_id)\n",
    "        correct_output = prompt_tuple[1]\n",
    "\n",
    "        if correct_output == current_output:\n",
    "            d['index'].append(idx)\n",
    "            d['prompts'].append(prompt_tuple[0])\n",
    "            d['correct_outputs'].append(correct_output)\n",
    "            d['current_outputs'].append(current_output)\n",
    "            d['correct_token_ids'].append(current_token_id.item())\n",
    "\n",
    "            # print('CORRECT')\n",
    "            # print(f'Prompt {idx}: {prompt_tuple[0]}')\n",
    "            # print(f'correct output = {correct_output}')\n",
    "            # print(f'current output = {current_output}')\n",
    "        else:\n",
    "            # finds the top logit instance of the correct answer token\n",
    "            sorted_out, token_idxs = torch.sort(\n",
    "                output_dict['logits'][0],\n",
    "                dim=-1,\n",
    "                descending=True\n",
    "            )\n",
    "\n",
    "            for j in range(0, sorted_out.shape[1]):\n",
    "                token = tokenizer.decode(token_idxs[0, j])\n",
    "                if correct_output == token:\n",
    "                    incorrect_d['correct_token_ids'].append(token_idxs[0, j].item())\n",
    "                    break\n",
    "\n",
    "            incorrect_d['index'].append(idx)\n",
    "            incorrect_d['prompts'].append(prompt_tuple[0])\n",
    "            incorrect_d['correct_outputs'].append(correct_output)\n",
    "            incorrect_d['current_outputs'].append(current_output)\n",
    "            incorrect_d['current_token_ids'].append(current_token_id.item())\n",
    "\n",
    "            print('INCORRECT')\n",
    "            print(f'Prompt {idx}: {prompt_tuple[0]}')\n",
    "            print(f'correct output = {correct_output}')\n",
    "            print(f'current output = {current_output}')\n",
    "            print('Full Generation: \"' + prompt_tuple[0] + filling + '\"')\n",
    "    return (d, incorrect_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af6ddb9376441af89b460ffff7fa5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prompts: 7\n",
      "))\n",
      "torch.Size([1, 32016])\n",
      "))\n",
      "torch.Size([1, 32016])\n",
      "))\n",
      "torch.Size([1, 32016])\n",
      "))\n",
      "torch.Size([1, 32016])\n",
      "))\n",
      "torch.Size([1, 32016])\n",
      "))\n",
      "torch.Size([1, 32016])\n",
      "))\n",
      "torch.Size([1, 32016])\n",
      "Total Prompts: 7\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = setup_model_and_tokenizer(index)\n",
    "model.to('cuda')\n",
    "clean_prompts = create_prompts(corrupted_flag=False)\n",
    "corrupted_prompts = create_prompts(corrupted_flag=True, corrupted_num_parens=corrupted_remove_parens)\n",
    "sys.exit(1)\n",
    "# ...\n",
    "d, incorrect_d = validate_prompts(model, tokenizer, prompts)\n",
    "\n",
    "\n",
    "# ...eeee\n",
    "corr_df = pd.DataFrame(d)\n",
    "incorr_df = pd.DataFrame(incorrect_d)\n",
    "corr_df.loc[:, \"correct_outputs\"].equals(corr_df.loc[:, \"current_outputs\"])\n",
    "\n",
    "corr_df.sort_values('correct_token_ids', inplace=True, kind='mergesort')\n",
    "incorr_df.sort_values('current_token_ids', inplace=True, kind='mergesort')\n",
    "\n",
    "print(corr_df.value_counts('correct_outputs'))\n",
    "print(incorr_df.value_counts('correct_outputs'))\n",
    "\n",
    "# convert to json and write out\n",
    "final_corr_dataset = corr_df.to_dict('records')\n",
    "final_incorr_dataset = incorr_df.to_dict('records')\n",
    "\n",
    "with open(\"../../data/codellama_incorrect_paren_data_v2.json\", \"w\") as f:\n",
    "    json.dump(final_incorr_dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"../../data/codellama_paren_data_v2.json\", \"w\") as f:\n",
    "    json.dump(final_corr_dataset, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
